:: Start ::
/home/013907062/OpenNMT-tf/scripts/HuggingFace/QuoQA-NLP/Final/trainenko.py:14: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  metric = load_metric("sacrebleu")
Using the latest cached version of the module from /home/013907062/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/556ba16a9634185dd1ea68395e0e474d6ee4de7e123fa701d577c6461f06032b (last modified on Thu Sep 14 16:32:47 2023) since it couldn't be found locally at sacrebleu, or remotely on the Hugging Face Hub.
[LOG2] metric done
LOG3 model name: QuoQA-NLP/KE-T5-En2Ko-Base
Map (num_proc=56):   0%|          | 0/3235 [00:00<?, ? examples/s]/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map (num_proc=56):   2%|â–         | 58/3235 [00:00<00:07, 441.58 examples/s]/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map (num_proc=56):   9%|â–‰         | 290/3235 [00:00<00:02, 1351.99 examples/s]/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map (num_proc=56):  32%|â–ˆâ–ˆâ–ˆâ–      | 1044/3235 [00:00<00:00, 2369.46 examples/s]Map (num_proc=56):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 2722/3235 [00:00<00:00, 6167.15 examples/s]Map (num_proc=56): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3235/3235 [00:00<00:00, 4465.67 examples/s]
Map (num_proc=56):   0%|          | 0/277 [00:00<?, ? examples/s]/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map (num_proc=56):   4%|â–Ž         | 10/277 [00:00<00:03, 86.65 examples/s]/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map (num_proc=56):  20%|â–ˆâ–‰        | 55/277 [00:00<00:00, 265.86 examples/s]/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3660: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.
  warnings.warn(
Map (num_proc=56):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 135/277 [00:00<00:00, 474.24 examples/s]Map (num_proc=56): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 812.38 examples/s]Map (num_proc=56): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 277/277 [00:00<00:00, 545.90 examples/s]
[LOG] tokenized_datasets done
[LOG] AutoModelForSeq2SeqLM done
[LOG] run_name KE-T5-En2Ko-Base-finetuned-en-to-ko
[LOG YJ] Trainer Ready DONE!
  0%|          | 0/500 [00:00<?, ?it/s]You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/500 [00:23<3:12:30, 23.15s/it]  0%|          | 2/500 [00:47<3:19:26, 24.03s/it]  1%|          | 3/500 [01:10<3:13:05, 23.31s/it]  1%|          | 4/500 [01:32<3:08:18, 22.78s/it]  1%|          | 5/500 [01:52<3:01:18, 21.98s/it]  1%|          | 6/500 [02:15<3:02:15, 22.14s/it]  1%|â–         | 7/500 [02:37<3:02:58, 22.27s/it]  2%|â–         | 8/500 [03:06<3:18:21, 24.19s/it]  2%|â–         | 9/500 [03:28<3:12:54, 23.57s/it]  2%|â–         | 10/500 [03:52<3:13:51, 23.74s/it]  2%|â–         | 11/500 [04:12<3:05:23, 22.75s/it]  2%|â–         | 12/500 [04:32<2:56:05, 21.65s/it]  3%|â–Ž         | 13/500 [04:55<3:00:36, 22.25s/it]  3%|â–Ž         | 14/500 [05:18<3:02:43, 22.56s/it]  3%|â–Ž         | 15/500 [05:44<3:09:56, 23.50s/it]  3%|â–Ž         | 16/500 [06:10<3:15:58, 24.29s/it]  3%|â–Ž         | 17/500 [06:36<3:18:54, 24.71s/it]  4%|â–Ž         | 18/500 [06:58<3:12:54, 24.01s/it]  4%|â–         | 19/500 [07:23<3:13:20, 24.12s/it]  4%|â–         | 20/500 [07:45<3:08:24, 23.55s/it]  4%|â–         | 21/500 [08:11<3:13:17, 24.21s/it]  4%|â–         | 22/500 [08:30<3:02:09, 22.87s/it]  5%|â–         | 23/500 [08:53<3:01:08, 22.79s/it]  5%|â–         | 24/500 [09:17<3:04:18, 23.23s/it]  5%|â–Œ         | 25/500 [09:40<3:03:53, 23.23s/it]
  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.50s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.69s/it][A                                                  
                                             [A  5%|â–Œ         | 25/500 [10:14<3:03:53, 23.23s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.69s/it][A
                                             [A  5%|â–Œ         | 26/500 [10:25<3:54:27, 29.68s/it]  5%|â–Œ         | 27/500 [10:49<3:39:12, 27.81s/it]  6%|â–Œ         | 28/500 [11:10<3:24:08, 25.95s/it]  6%|â–Œ         | 29/500 [11:33<3:16:24, 25.02s/it]  6%|â–Œ         | 30/500 [11:59<3:17:48, 25.25s/it]  6%|â–Œ         | 31/500 [12:23<3:14:37, 24.90s/it]  6%|â–‹         | 32/500 [12:52<3:23:36, 26.10s/it]  7%|â–‹         | 33/500 [13:15<3:16:10, 25.20s/it]  7%|â–‹         | 34/500 [13:39<3:12:28, 24.78s/it]  7%|â–‹         | 35/500 [14:03<3:10:20, 24.56s/it]  7%|â–‹         | 36/500 [14:24<3:01:54, 23.52s/it]  7%|â–‹         | 37/500 [14:46<2:58:05, 23.08s/it]  8%|â–Š         | 38/500 [15:10<2:59:17, 23.29s/it]  8%|â–Š         | 39/500 [15:31<2:54:49, 22.75s/it]  8%|â–Š         | 40/500 [15:58<3:02:29, 23.80s/it]  8%|â–Š         | 41/500 [16:20<2:59:27, 23.46s/it]  8%|â–Š         | 42/500 [16:42<2:54:40, 22.88s/it]  9%|â–Š         | 43/500 [17:09<3:03:58, 24.15s/it]  9%|â–‰         | 44/500 [17:31<3:00:08, 23.70s/it]  9%|â–‰         | 45/500 [17:55<2:58:15, 23.51s/it]  9%|â–‰         | 46/500 [18:21<3:03:34, 24.26s/it]  9%|â–‰         | 47/500 [18:43<2:58:59, 23.71s/it] 10%|â–‰         | 48/500 [19:06<2:57:06, 23.51s/it] 10%|â–‰         | 49/500 [19:29<2:54:26, 23.21s/it] 10%|â–ˆ         | 50/500 [19:51<2:53:12, 23.09s/it] 10%|â–ˆ         | 51/500 [20:09<2:40:31, 21.45s/it]{'eval_loss': 2.615086317062378, 'eval_bleu': 10.4611, 'eval_gen_len': 13.0289, 'eval_runtime': 28.2305, 'eval_samples_per_second': 9.812, 'eval_steps_per_second': 0.106, 'epoch': 0.98}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.79s/it][A                                                  
                                             [A 10%|â–ˆ         | 51/500 [20:38<2:40:31, 21.45s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.79s/it][A
                                             [A 10%|â–ˆ         | 52/500 [21:01<3:47:54, 30.52s/it] 11%|â–ˆ         | 53/500 [21:27<3:37:18, 29.17s/it] 11%|â–ˆ         | 54/500 [21:49<3:22:29, 27.24s/it] 11%|â–ˆ         | 55/500 [22:14<3:16:00, 26.43s/it] 11%|â–ˆ         | 56/500 [22:38<3:09:23, 25.59s/it] 11%|â–ˆâ–        | 57/500 [23:02<3:06:50, 25.31s/it] 12%|â–ˆâ–        | 58/500 [23:27<3:06:07, 25.26s/it] 12%|â–ˆâ–        | 59/500 [23:50<3:00:39, 24.58s/it] 12%|â–ˆâ–        | 60/500 [24:12<2:53:46, 23.70s/it] 12%|â–ˆâ–        | 61/500 [24:34<2:49:20, 23.15s/it] 12%|â–ˆâ–        | 62/500 [24:57<2:49:26, 23.21s/it] 13%|â–ˆâ–Ž        | 63/500 [25:21<2:49:26, 23.26s/it] 13%|â–ˆâ–Ž        | 64/500 [25:49<2:59:23, 24.69s/it] 13%|â–ˆâ–Ž        | 65/500 [26:16<3:05:10, 25.54s/it] 13%|â–ˆâ–Ž        | 66/500 [26:37<2:54:54, 24.18s/it] 13%|â–ˆâ–Ž        | 67/500 [27:01<2:53:26, 24.03s/it] 14%|â–ˆâ–Ž        | 68/500 [27:28<3:00:08, 25.02s/it] 14%|â–ˆâ–        | 69/500 [27:52<2:57:16, 24.68s/it] 14%|â–ˆâ–        | 70/500 [28:15<2:52:08, 24.02s/it] 14%|â–ˆâ–        | 71/500 [28:36<2:45:37, 23.17s/it] 14%|â–ˆâ–        | 72/500 [28:57<2:40:10, 22.45s/it] 15%|â–ˆâ–        | 73/500 [29:21<2:44:25, 23.10s/it] 15%|â–ˆâ–        | 74/500 [29:46<2:48:47, 23.77s/it] 15%|â–ˆâ–Œ        | 75/500 [30:10<2:48:18, 23.76s/it] 15%|â–ˆâ–Œ        | 76/500 [30:36<2:51:20, 24.25s/it]{'eval_loss': 2.4004125595092773, 'eval_bleu': 11.4119, 'eval_gen_len': 13.0108, 'eval_runtime': 29.1282, 'eval_samples_per_second': 9.51, 'eval_steps_per_second': 0.103, 'epoch': 2.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.74s/it][A                                                  
                                             [A 15%|â–ˆâ–Œ        | 76/500 [31:10<2:51:20, 24.25s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.74s/it][A
                                             [A 15%|â–ˆâ–Œ        | 77/500 [31:23<3:40:59, 31.35s/it] 16%|â–ˆâ–Œ        | 78/500 [31:47<3:24:39, 29.10s/it] 16%|â–ˆâ–Œ        | 79/500 [32:14<3:19:48, 28.48s/it] 16%|â–ˆâ–Œ        | 80/500 [32:39<3:11:12, 27.32s/it] 16%|â–ˆâ–Œ        | 81/500 [33:01<2:59:24, 25.69s/it] 16%|â–ˆâ–‹        | 82/500 [33:26<2:56:46, 25.37s/it] 17%|â–ˆâ–‹        | 83/500 [33:50<2:55:28, 25.25s/it] 17%|â–ˆâ–‹        | 84/500 [34:13<2:48:33, 24.31s/it] 17%|â–ˆâ–‹        | 85/500 [34:34<2:42:18, 23.47s/it] 17%|â–ˆâ–‹        | 86/500 [34:56<2:39:14, 23.08s/it] 17%|â–ˆâ–‹        | 87/500 [35:17<2:34:34, 22.46s/it] 18%|â–ˆâ–Š        | 88/500 [35:42<2:39:50, 23.28s/it] 18%|â–ˆâ–Š        | 89/500 [36:07<2:41:27, 23.57s/it] 18%|â–ˆâ–Š        | 90/500 [36:30<2:41:12, 23.59s/it] 18%|â–ˆâ–Š        | 91/500 [36:53<2:39:50, 23.45s/it] 18%|â–ˆâ–Š        | 92/500 [37:17<2:40:03, 23.54s/it] 19%|â–ˆâ–Š        | 93/500 [37:40<2:37:25, 23.21s/it] 19%|â–ˆâ–‰        | 94/500 [38:04<2:39:12, 23.53s/it] 19%|â–ˆâ–‰        | 95/500 [38:26<2:36:06, 23.13s/it] 19%|â–ˆâ–‰        | 96/500 [38:56<2:49:08, 25.12s/it] 19%|â–ˆâ–‰        | 97/500 [39:19<2:44:39, 24.51s/it] 20%|â–ˆâ–‰        | 98/500 [39:42<2:41:22, 24.09s/it] 20%|â–ˆâ–‰        | 99/500 [40:05<2:39:22, 23.85s/it] 20%|â–ˆâ–ˆ        | 100/500 [40:25<2:30:42, 22.61s/it] 20%|â–ˆâ–ˆ        | 101/500 [40:52<2:38:37, 23.85s/it] 20%|â–ˆâ–ˆ        | 102/500 [41:09<2:23:57, 21.70s/it]{'eval_loss': 2.2965848445892334, 'eval_bleu': 11.8683, 'eval_gen_len': 12.9278, 'eval_runtime': 29.0153, 'eval_samples_per_second': 9.547, 'eval_steps_per_second': 0.103, 'epoch': 2.98}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.73s/it][A                                                   
                                             [A 20%|â–ˆâ–ˆ        | 102/500 [41:38<2:23:57, 21.70s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.73s/it][A
                                             [A 21%|â–ˆâ–ˆ        | 103/500 [42:01<3:24:36, 30.92s/it] 21%|â–ˆâ–ˆ        | 104/500 [42:22<3:05:25, 28.10s/it] 21%|â–ˆâ–ˆ        | 105/500 [42:49<3:01:50, 27.62s/it] 21%|â–ˆâ–ˆ        | 106/500 [43:13<2:53:25, 26.41s/it] 21%|â–ˆâ–ˆâ–       | 107/500 [43:43<3:00:06, 27.50s/it] 22%|â–ˆâ–ˆâ–       | 108/500 [44:04<2:47:24, 25.62s/it] 22%|â–ˆâ–ˆâ–       | 109/500 [44:27<2:42:30, 24.94s/it] 22%|â–ˆâ–ˆâ–       | 110/500 [44:51<2:40:14, 24.65s/it] 22%|â–ˆâ–ˆâ–       | 111/500 [45:17<2:42:42, 25.10s/it] 22%|â–ˆâ–ˆâ–       | 112/500 [45:44<2:44:27, 25.43s/it] 23%|â–ˆâ–ˆâ–Ž       | 113/500 [46:07<2:40:24, 24.87s/it] 23%|â–ˆâ–ˆâ–Ž       | 114/500 [46:28<2:32:17, 23.67s/it] 23%|â–ˆâ–ˆâ–Ž       | 115/500 [46:55<2:38:15, 24.66s/it] 23%|â–ˆâ–ˆâ–Ž       | 116/500 [47:19<2:36:04, 24.39s/it] 23%|â–ˆâ–ˆâ–Ž       | 117/500 [47:41<2:32:00, 23.81s/it] 24%|â–ˆâ–ˆâ–Ž       | 118/500 [48:04<2:30:18, 23.61s/it] 24%|â–ˆâ–ˆâ–       | 119/500 [48:28<2:29:43, 23.58s/it] 24%|â–ˆâ–ˆâ–       | 120/500 [48:50<2:26:23, 23.12s/it] 24%|â–ˆâ–ˆâ–       | 121/500 [49:14<2:27:47, 23.40s/it] 24%|â–ˆâ–ˆâ–       | 122/500 [49:38<2:28:53, 23.63s/it] 25%|â–ˆâ–ˆâ–       | 123/500 [49:58<2:21:50, 22.57s/it] 25%|â–ˆâ–ˆâ–       | 124/500 [50:22<2:24:16, 23.02s/it] 25%|â–ˆâ–ˆâ–Œ       | 125/500 [50:45<2:23:56, 23.03s/it] 25%|â–ˆâ–ˆâ–Œ       | 126/500 [51:07<2:21:24, 22.69s/it] 25%|â–ˆâ–ˆâ–Œ       | 127/500 [51:29<2:19:57, 22.51s/it]{'eval_loss': 2.216395854949951, 'eval_bleu': 14.033, 'eval_gen_len': 12.9603, 'eval_runtime': 29.0701, 'eval_samples_per_second': 9.529, 'eval_steps_per_second': 0.103, 'epoch': 4.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.63s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.74s/it][A                                                   
                                             [A 25%|â–ˆâ–ˆâ–Œ       | 127/500 [52:05<2:19:57, 22.51s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.74s/it][A
                                             [A 26%|â–ˆâ–ˆâ–Œ       | 128/500 [52:17<3:07:12, 30.19s/it] 26%|â–ˆâ–ˆâ–Œ       | 129/500 [52:43<2:58:23, 28.85s/it] 26%|â–ˆâ–ˆâ–Œ       | 130/500 [53:06<2:46:21, 26.98s/it] 26%|â–ˆâ–ˆâ–Œ       | 131/500 [53:28<2:37:39, 25.64s/it] 26%|â–ˆâ–ˆâ–‹       | 132/500 [53:53<2:35:42, 25.39s/it] 27%|â–ˆâ–ˆâ–‹       | 133/500 [54:15<2:28:23, 24.26s/it] 27%|â–ˆâ–ˆâ–‹       | 134/500 [54:37<2:24:04, 23.62s/it] 27%|â–ˆâ–ˆâ–‹       | 135/500 [55:00<2:22:41, 23.46s/it] 27%|â–ˆâ–ˆâ–‹       | 136/500 [55:22<2:20:03, 23.09s/it] 27%|â–ˆâ–ˆâ–‹       | 137/500 [55:46<2:20:43, 23.26s/it] 28%|â–ˆâ–ˆâ–Š       | 138/500 [56:07<2:17:16, 22.75s/it] 28%|â–ˆâ–ˆâ–Š       | 139/500 [56:31<2:18:26, 23.01s/it] 28%|â–ˆâ–ˆâ–Š       | 140/500 [56:55<2:19:11, 23.20s/it] 28%|â–ˆâ–ˆâ–Š       | 141/500 [57:18<2:19:04, 23.24s/it] 28%|â–ˆâ–ˆâ–Š       | 142/500 [57:45<2:25:20, 24.36s/it] 29%|â–ˆâ–ˆâ–Š       | 143/500 [58:06<2:19:22, 23.43s/it] 29%|â–ˆâ–ˆâ–‰       | 144/500 [58:29<2:18:27, 23.34s/it] 29%|â–ˆâ–ˆâ–‰       | 145/500 [58:51<2:15:38, 22.93s/it] 29%|â–ˆâ–ˆâ–‰       | 146/500 [59:16<2:18:18, 23.44s/it] 29%|â–ˆâ–ˆâ–‰       | 147/500 [59:44<2:26:06, 24.84s/it] 30%|â–ˆâ–ˆâ–‰       | 148/500 [1:00:11<2:29:22, 25.46s/it] 30%|â–ˆâ–ˆâ–‰       | 149/500 [1:00:35<2:26:14, 25.00s/it] 30%|â–ˆâ–ˆâ–ˆ       | 150/500 [1:00:58<2:22:32, 24.44s/it]                                                      30%|â–ˆâ–ˆâ–ˆ       | 150/500 [1:00:58<2:22:32, 24.44s/it] 30%|â–ˆâ–ˆâ–ˆ       | 151/500 [1:01:29<2:34:34, 26.57s/it] 30%|â–ˆâ–ˆâ–ˆ       | 152/500 [1:01:53<2:28:39, 25.63s/it] 31%|â–ˆâ–ˆâ–ˆ       | 153/500 [1:02:11<2:15:17, 23.39s/it]{'eval_loss': 2.207322835922241, 'eval_bleu': 14.3396, 'eval_gen_len': 12.8773, 'eval_runtime': 29.0187, 'eval_samples_per_second': 9.546, 'eval_steps_per_second': 0.103, 'epoch': 4.98}
{'loss': 2.1228, 'learning_rate': 0.00035, 'epoch': 5.88}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.72s/it][A                                                     
                                             [A 31%|â–ˆâ–ˆâ–ˆ       | 153/500 [1:02:40<2:15:17, 23.39s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.72s/it][A
                                             [A 31%|â–ˆâ–ˆâ–ˆ       | 154/500 [1:03:03<3:04:13, 31.95s/it] 31%|â–ˆâ–ˆâ–ˆ       | 155/500 [1:03:26<2:47:58, 29.21s/it] 31%|â–ˆâ–ˆâ–ˆ       | 156/500 [1:03:53<2:43:09, 28.46s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 157/500 [1:04:15<2:33:09, 26.79s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 158/500 [1:04:38<2:25:28, 25.52s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 159/500 [1:05:02<2:21:46, 24.95s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 160/500 [1:05:21<2:11:28, 23.20s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 161/500 [1:05:43<2:09:37, 22.94s/it] 32%|â–ˆâ–ˆâ–ˆâ–      | 162/500 [1:06:08<2:12:45, 23.57s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 163/500 [1:06:33<2:15:16, 24.08s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 164/500 [1:06:59<2:17:04, 24.48s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 165/500 [1:07:21<2:13:12, 23.86s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 166/500 [1:07:45<2:13:05, 23.91s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 167/500 [1:08:07<2:08:33, 23.16s/it] 34%|â–ˆâ–ˆâ–ˆâ–Ž      | 168/500 [1:08:28<2:04:24, 22.48s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 169/500 [1:08:50<2:03:24, 22.37s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 170/500 [1:09:11<2:02:12, 22.22s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 171/500 [1:09:39<2:10:33, 23.81s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 172/500 [1:10:02<2:08:11, 23.45s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 173/500 [1:10:27<2:10:51, 24.01s/it] 35%|â–ˆâ–ˆâ–ˆâ–      | 174/500 [1:10:54<2:15:05, 24.86s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 175/500 [1:11:18<2:13:06, 24.57s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 176/500 [1:11:42<2:12:30, 24.54s/it] 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 177/500 [1:12:07<2:13:22, 24.77s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 178/500 [1:12:31<2:11:31, 24.51s/it]{'eval_loss': 2.2287070751190186, 'eval_bleu': 15.6565, 'eval_gen_len': 12.7906, 'eval_runtime': 28.9939, 'eval_samples_per_second': 9.554, 'eval_steps_per_second': 0.103, 'epoch': 6.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.79s/it][A                                                     
                                             [A 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 178/500 [1:13:06<2:11:31, 24.51s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.79s/it][A
                                             [A 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 179/500 [1:13:22<2:53:05, 32.35s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 180/500 [1:13:47<2:40:36, 30.12s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 181/500 [1:14:10<2:29:21, 28.09s/it] 36%|â–ˆâ–ˆâ–ˆâ–‹      | 182/500 [1:14:32<2:19:15, 26.28s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 183/500 [1:14:56<2:14:10, 25.40s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 184/500 [1:15:19<2:10:00, 24.69s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 185/500 [1:15:43<2:09:00, 24.57s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 186/500 [1:16:06<2:06:41, 24.21s/it] 37%|â–ˆâ–ˆâ–ˆâ–‹      | 187/500 [1:16:29<2:04:30, 23.87s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 188/500 [1:16:54<2:05:40, 24.17s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 189/500 [1:17:17<2:03:12, 23.77s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 190/500 [1:17:41<2:02:24, 23.69s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 191/500 [1:18:05<2:02:55, 23.87s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 192/500 [1:18:26<1:57:50, 22.96s/it] 39%|â–ˆâ–ˆâ–ˆâ–Š      | 193/500 [1:18:48<1:56:42, 22.81s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 194/500 [1:19:09<1:53:42, 22.30s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 195/500 [1:19:32<1:54:03, 22.44s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 196/500 [1:19:54<1:52:13, 22.15s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 197/500 [1:20:19<1:56:14, 23.02s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 198/500 [1:20:44<1:59:02, 23.65s/it] 40%|â–ˆâ–ˆâ–ˆâ–‰      | 199/500 [1:21:13<2:06:50, 25.28s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 200/500 [1:21:35<2:01:43, 24.34s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 201/500 [1:21:58<1:59:31, 23.99s/it] 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 202/500 [1:22:26<2:05:23, 25.25s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 203/500 [1:22:47<1:58:55, 24.02s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 204/500 [1:23:05<1:48:40, 22.03s/it]{'eval_loss': 2.2337794303894043, 'eval_bleu': 15.6202, 'eval_gen_len': 12.9386, 'eval_runtime': 29.0445, 'eval_samples_per_second': 9.537, 'eval_steps_per_second': 0.103, 'epoch': 6.98}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.70s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.80s/it][A                                                     
                                             [A 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 204/500 [1:23:34<1:48:40, 22.03s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.80s/it][A
                                             [A 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 205/500 [1:23:59<2:35:03, 31.54s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 206/500 [1:24:21<2:20:34, 28.69s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 207/500 [1:24:44<2:11:44, 26.98s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 208/500 [1:25:10<2:10:07, 26.74s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 209/500 [1:25:33<2:03:54, 25.55s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 210/500 [1:25:54<1:57:33, 24.32s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 211/500 [1:26:17<1:54:51, 23.85s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 212/500 [1:26:43<1:58:08, 24.61s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 213/500 [1:27:06<1:55:35, 24.16s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 214/500 [1:27:29<1:52:26, 23.59s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 215/500 [1:27:57<1:58:36, 24.97s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 216/500 [1:28:21<1:57:17, 24.78s/it] 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 217/500 [1:28:44<1:54:10, 24.21s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 218/500 [1:29:09<1:54:52, 24.44s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 219/500 [1:29:31<1:51:50, 23.88s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 220/500 [1:29:57<1:54:08, 24.46s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 221/500 [1:30:23<1:55:59, 24.94s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 222/500 [1:30:45<1:51:07, 23.99s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 223/500 [1:31:06<1:46:38, 23.10s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 224/500 [1:31:35<1:54:05, 24.80s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 225/500 [1:31:55<1:47:29, 23.45s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 226/500 [1:32:18<1:46:25, 23.31s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 227/500 [1:32:41<1:45:17, 23.14s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 228/500 [1:33:04<1:45:04, 23.18s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 229/500 [1:33:28<1:45:36, 23.38s/it]{'eval_loss': 2.270028591156006, 'eval_bleu': 15.603, 'eval_gen_len': 12.7978, 'eval_runtime': 29.1, 'eval_samples_per_second': 9.519, 'eval_steps_per_second': 0.103, 'epoch': 8.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.64s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.79s/it][A                                                     
                                             [A 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 229/500 [1:34:03<1:45:36, 23.38s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.79s/it][A
                                             [A 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 230/500 [1:34:16<2:18:19, 30.74s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 231/500 [1:34:39<2:08:04, 28.57s/it] 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 232/500 [1:35:03<2:00:12, 26.91s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 233/500 [1:35:30<2:00:41, 27.12s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 234/500 [1:35:53<1:54:12, 25.76s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 235/500 [1:36:17<1:51:31, 25.25s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 236/500 [1:36:41<1:49:11, 24.82s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 237/500 [1:37:04<1:46:59, 24.41s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 238/500 [1:37:26<1:43:31, 23.71s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 239/500 [1:37:48<1:41:14, 23.28s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 240/500 [1:38:12<1:41:08, 23.34s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 241/500 [1:38:34<1:39:09, 22.97s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 242/500 [1:39:01<1:43:34, 24.09s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 243/500 [1:39:22<1:40:08, 23.38s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 244/500 [1:39:43<1:36:44, 22.67s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 245/500 [1:40:03<1:32:56, 21.87s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 246/500 [1:40:33<1:41:49, 24.05s/it] 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 247/500 [1:40:59<1:44:58, 24.89s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 248/500 [1:41:21<1:40:33, 23.94s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 249/500 [1:41:44<1:38:41, 23.59s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 250/500 [1:42:04<1:33:31, 22.45s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 251/500 [1:42:27<1:34:45, 22.83s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 252/500 [1:42:50<1:34:02, 22.75s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 253/500 [1:43:16<1:37:09, 23.60s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 254/500 [1:43:38<1:35:13, 23.23s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 255/500 [1:43:55<1:27:43, 21.48s/it]{'eval_loss': 2.3323328495025635, 'eval_bleu': 15.9912, 'eval_gen_len': 12.9928, 'eval_runtime': 28.995, 'eval_samples_per_second': 9.553, 'eval_steps_per_second': 0.103, 'epoch': 8.98}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.77s/it][A                                                     
                                             [A 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 255/500 [1:44:24<1:27:43, 21.48s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.77s/it][A
                                             [A 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 256/500 [1:44:49<2:06:14, 31.04s/it] 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 257/500 [1:45:13<1:57:20, 28.97s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 258/500 [1:45:37<1:51:29, 27.64s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 259/500 [1:46:01<1:46:22, 26.48s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 260/500 [1:46:25<1:42:49, 25.71s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 261/500 [1:46:47<1:37:58, 24.59s/it] 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 262/500 [1:47:15<1:41:07, 25.49s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 263/500 [1:47:37<1:36:48, 24.51s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 264/500 [1:47:58<1:32:45, 23.58s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 265/500 [1:48:25<1:36:32, 24.65s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 266/500 [1:48:49<1:35:15, 24.42s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 267/500 [1:49:12<1:32:26, 23.80s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 268/500 [1:49:35<1:32:00, 23.79s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 269/500 [1:49:54<1:26:01, 22.35s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 270/500 [1:50:20<1:29:15, 23.28s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 271/500 [1:50:47<1:32:56, 24.35s/it] 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 272/500 [1:51:10<1:31:16, 24.02s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 273/500 [1:51:31<1:27:21, 23.09s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 274/500 [1:51:56<1:28:44, 23.56s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 275/500 [1:52:19<1:28:19, 23.56s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 276/500 [1:52:47<1:32:51, 24.87s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 277/500 [1:53:09<1:29:38, 24.12s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 278/500 [1:53:33<1:29:13, 24.11s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 279/500 [1:53:53<1:23:25, 22.65s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 280/500 [1:54:16<1:23:44, 22.84s/it]{'eval_loss': 2.3546242713928223, 'eval_bleu': 17.146, 'eval_gen_len': 12.9097, 'eval_runtime': 28.8367, 'eval_samples_per_second': 9.606, 'eval_steps_per_second': 0.104, 'epoch': 10.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.69s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.80s/it][A                                                     
                                             [A 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 280/500 [1:54:51<1:23:44, 22.84s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.80s/it][A
                                             [A 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 281/500 [1:55:04<1:50:39, 30.32s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 282/500 [1:55:28<1:43:47, 28.57s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 283/500 [1:55:57<1:43:26, 28.60s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 284/500 [1:56:20<1:37:06, 26.97s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 285/500 [1:56:44<1:32:50, 25.91s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 286/500 [1:57:10<1:33:28, 26.21s/it] 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 287/500 [1:57:36<1:32:22, 26.02s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 288/500 [1:57:59<1:28:32, 25.06s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 289/500 [1:58:22<1:26:25, 24.57s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 290/500 [1:58:45<1:23:50, 23.95s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 291/500 [1:59:09<1:24:10, 24.16s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 292/500 [1:59:34<1:23:41, 24.14s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 293/500 [1:59:54<1:19:59, 23.19s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 294/500 [2:00:18<1:19:31, 23.16s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 295/500 [2:00:41<1:18:56, 23.11s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 296/500 [2:01:05<1:20:09, 23.57s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 297/500 [2:01:33<1:24:28, 24.97s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 298/500 [2:01:58<1:23:24, 24.77s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 299/500 [2:02:19<1:19:48, 23.82s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 300/500 [2:02:42<1:18:32, 23.56s/it]                                                      60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 300/500 [2:02:42<1:18:32, 23.56s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 301/500 [2:03:13<1:25:19, 25.73s/it] 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 302/500 [2:03:37<1:22:37, 25.04s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 303/500 [2:04:00<1:20:29, 24.51s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 304/500 [2:04:23<1:19:10, 24.24s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 305/500 [2:04:49<1:20:33, 24.78s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 306/500 [2:05:08<1:14:27, 23.03s/it]{'eval_loss': 2.437530994415283, 'eval_bleu': 16.981, 'eval_gen_len': 12.9783, 'eval_runtime': 29.1171, 'eval_samples_per_second': 9.513, 'eval_steps_per_second': 0.103, 'epoch': 10.98}
{'loss': 1.115, 'learning_rate': 0.0002, 'epoch': 11.76}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.78s/it][A                                                     
                                             [A 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 306/500 [2:05:38<1:14:27, 23.03s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.78s/it][A
                                             [A 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 307/500 [2:06:00<1:41:43, 31.62s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 308/500 [2:06:23<1:33:16, 29.15s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 309/500 [2:06:46<1:26:37, 27.21s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 310/500 [2:07:09<1:21:57, 25.88s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 311/500 [2:07:34<1:20:42, 25.62s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 312/500 [2:07:58<1:18:52, 25.17s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 313/500 [2:08:24<1:19:19, 25.45s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 314/500 [2:08:45<1:14:55, 24.17s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 315/500 [2:09:08<1:13:04, 23.70s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 316/500 [2:09:31<1:12:17, 23.57s/it] 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 317/500 [2:09:57<1:13:27, 24.08s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 318/500 [2:10:20<1:12:04, 23.76s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 319/500 [2:10:44<1:11:52, 23.82s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 320/500 [2:11:08<1:12:04, 24.02s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 321/500 [2:11:33<1:12:13, 24.21s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 322/500 [2:12:00<1:14:41, 25.18s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 323/500 [2:12:25<1:14:25, 25.23s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 324/500 [2:12:48<1:11:17, 24.30s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 325/500 [2:13:14<1:12:19, 24.80s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 326/500 [2:13:37<1:10:57, 24.47s/it] 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 327/500 [2:13:59<1:08:31, 23.77s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 328/500 [2:14:24<1:08:30, 23.90s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 329/500 [2:14:46<1:06:58, 23.50s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 330/500 [2:15:08<1:05:21, 23.07s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 331/500 [2:15:29<1:03:05, 22.40s/it]{'eval_loss': 2.4705748558044434, 'eval_bleu': 16.5384, 'eval_gen_len': 12.9314, 'eval_runtime': 29.1892, 'eval_samples_per_second': 9.49, 'eval_steps_per_second': 0.103, 'epoch': 12.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.75s/it][A                                                     
                                             [A 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 331/500 [2:16:06<1:03:05, 22.40s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.75s/it][A
                                             [A 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 332/500 [2:16:20<1:27:05, 31.11s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 333/500 [2:16:43<1:19:37, 28.61s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 334/500 [2:17:08<1:16:13, 27.55s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 335/500 [2:17:31<1:12:05, 26.22s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 336/500 [2:17:54<1:08:18, 24.99s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 337/500 [2:18:15<1:05:02, 23.94s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 338/500 [2:18:38<1:03:53, 23.66s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 339/500 [2:19:00<1:01:51, 23.05s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 340/500 [2:19:23<1:01:42, 23.14s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 341/500 [2:19:45<1:00:46, 22.93s/it] 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 342/500 [2:20:10<1:01:58, 23.54s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 343/500 [2:20:36<1:03:24, 24.23s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 344/500 [2:21:00<1:02:40, 24.11s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 345/500 [2:21:23<1:01:05, 23.65s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 346/500 [2:21:47<1:00:51, 23.71s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 347/500 [2:22:09<59:32, 23.35s/it]   70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 348/500 [2:22:30<57:33, 22.72s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 349/500 [2:22:58<1:00:46, 24.15s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 350/500 [2:23:20<58:54, 23.56s/it]   70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 351/500 [2:23:43<58:10, 23.42s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 352/500 [2:24:06<57:08, 23.17s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 353/500 [2:24:30<57:52, 23.62s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 354/500 [2:24:57<59:38, 24.51s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 355/500 [2:25:19<57:30, 23.80s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 356/500 [2:25:43<56:58, 23.74s/it] 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 357/500 [2:26:01<52:51, 22.18s/it]{'eval_loss': 2.5126023292541504, 'eval_bleu': 17.1724, 'eval_gen_len': 12.9856, 'eval_runtime': 28.9429, 'eval_samples_per_second': 9.571, 'eval_steps_per_second': 0.104, 'epoch': 12.98}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.76s/it][A                                                   
                                             [A 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 357/500 [2:26:30<52:51, 22.18s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.76s/it][A
                                             [A 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 358/500 [2:26:52<1:12:46, 30.75s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 359/500 [2:27:15<1:06:30, 28.30s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 360/500 [2:27:37<1:01:56, 26.55s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 361/500 [2:28:00<59:17, 25.60s/it]   72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 362/500 [2:28:24<57:39, 25.07s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 363/500 [2:28:47<55:26, 24.28s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 364/500 [2:29:10<54:12, 23.91s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 365/500 [2:29:32<52:53, 23.51s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 366/500 [2:29:59<54:47, 24.53s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 367/500 [2:30:29<57:42, 26.03s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 368/500 [2:30:52<55:43, 25.33s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 369/500 [2:31:16<54:05, 24.78s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 370/500 [2:31:36<50:50, 23.47s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 371/500 [2:31:58<49:33, 23.05s/it] 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 372/500 [2:32:22<49:15, 23.09s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 373/500 [2:32:51<52:38, 24.87s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 374/500 [2:33:19<54:30, 25.95s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 375/500 [2:33:40<50:43, 24.35s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 376/500 [2:34:05<51:07, 24.73s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 377/500 [2:34:28<49:33, 24.18s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 378/500 [2:34:51<48:29, 23.85s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 379/500 [2:35:16<48:51, 24.23s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 380/500 [2:35:38<47:02, 23.52s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 381/500 [2:36:00<45:48, 23.10s/it] 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 382/500 [2:36:23<44:53, 22.82s/it]{'eval_loss': 2.5508110523223877, 'eval_bleu': 16.7071, 'eval_gen_len': 13.0108, 'eval_runtime': 28.9172, 'eval_samples_per_second': 9.579, 'eval_steps_per_second': 0.104, 'epoch': 14.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.76s/it][A                                                   
                                             [A 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 382/500 [2:36:57<44:53, 22.82s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.76s/it][A
                                             [A 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 383/500 [2:37:08<57:53, 29.69s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 384/500 [2:37:32<54:01, 27.95s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 385/500 [2:37:56<51:27, 26.85s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 386/500 [2:38:18<47:59, 25.26s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 387/500 [2:38:46<48:54, 25.97s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 388/500 [2:39:11<48:26, 25.95s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 389/500 [2:39:35<46:34, 25.18s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 390/500 [2:39:56<43:52, 23.93s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 391/500 [2:40:18<42:39, 23.48s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 392/500 [2:40:46<44:22, 24.65s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 393/500 [2:41:10<43:31, 24.41s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 394/500 [2:41:33<42:34, 24.10s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 395/500 [2:41:55<41:11, 23.53s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 396/500 [2:42:16<39:30, 22.80s/it] 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 397/500 [2:42:39<39:02, 22.74s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 398/500 [2:43:04<39:40, 23.34s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 399/500 [2:43:26<38:40, 22.98s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 400/500 [2:43:50<38:50, 23.31s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 401/500 [2:44:14<39:04, 23.68s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 402/500 [2:44:38<38:44, 23.72s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 403/500 [2:45:04<39:11, 24.24s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 404/500 [2:45:28<38:45, 24.22s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 405/500 [2:45:54<39:31, 24.96s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 406/500 [2:46:13<36:11, 23.10s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 407/500 [2:46:35<35:00, 22.59s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 408/500 [2:46:53<32:50, 21.42s/it]{'eval_loss': 2.5586304664611816, 'eval_bleu': 17.2886, 'eval_gen_len': 13.0903, 'eval_runtime': 28.856, 'eval_samples_per_second': 9.599, 'eval_steps_per_second': 0.104, 'epoch': 14.98}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.62s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.76s/it][A                                                   
                                             [A 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 408/500 [2:47:22<32:50, 21.42s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.76s/it][A
                                             [A 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 409/500 [2:47:44<45:46, 30.19s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 410/500 [2:48:11<43:51, 29.24s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 411/500 [2:48:36<41:24, 27.92s/it] 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 412/500 [2:48:57<38:07, 26.00s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 413/500 [2:49:19<35:42, 24.63s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 414/500 [2:49:43<35:14, 24.59s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 415/500 [2:50:04<33:11, 23.42s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 416/500 [2:50:27<32:40, 23.33s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 417/500 [2:50:50<32:11, 23.27s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 418/500 [2:51:13<31:25, 23.00s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 419/500 [2:51:39<32:17, 23.92s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 420/500 [2:52:02<31:35, 23.70s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 421/500 [2:52:29<32:42, 24.84s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 422/500 [2:52:53<31:55, 24.56s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 423/500 [2:53:16<30:46, 23.99s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 424/500 [2:53:40<30:26, 24.03s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 425/500 [2:54:03<29:35, 23.67s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 426/500 [2:54:29<30:12, 24.50s/it] 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 427/500 [2:54:53<29:27, 24.21s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 428/500 [2:55:24<31:25, 26.18s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 429/500 [2:55:45<29:26, 24.88s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 430/500 [2:56:11<29:23, 25.19s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 431/500 [2:56:34<28:00, 24.36s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 432/500 [2:56:55<26:31, 23.40s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 433/500 [2:57:18<26:09, 23.43s/it]{'eval_loss': 2.6157479286193848, 'eval_bleu': 17.5393, 'eval_gen_len': 13.1155, 'eval_runtime': 28.9763, 'eval_samples_per_second': 9.56, 'eval_steps_per_second': 0.104, 'epoch': 16.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.65s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.77s/it][A                                                   
                                             [A 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 433/500 [2:57:52<26:09, 23.43s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.77s/it][A
                                             [A 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 434/500 [2:58:03<32:43, 29.75s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 435/500 [2:58:28<30:47, 28.42s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 436/500 [2:58:56<30:10, 28.28s/it] 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 437/500 [2:59:18<27:34, 26.26s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 438/500 [2:59:40<25:56, 25.10s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 439/500 [3:00:03<24:42, 24.31s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 440/500 [3:00:25<23:36, 23.60s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 441/500 [3:00:49<23:28, 23.86s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 442/500 [3:01:12<22:56, 23.73s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 443/500 [3:01:39<23:13, 24.44s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 444/500 [3:02:01<22:09, 23.74s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 445/500 [3:02:24<21:38, 23.61s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 446/500 [3:02:50<21:53, 24.32s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 447/500 [3:03:12<20:49, 23.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 448/500 [3:03:35<20:17, 23.41s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 449/500 [3:04:02<20:45, 24.43s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 450/500 [3:04:27<20:29, 24.58s/it]                                                    90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 450/500 [3:04:27<20:29, 24.58s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 451/500 [3:05:02<22:46, 27.88s/it] 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 452/500 [3:05:24<20:53, 26.11s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 453/500 [3:05:51<20:36, 26.31s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 454/500 [3:06:14<19:28, 25.41s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 455/500 [3:06:37<18:31, 24.70s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 456/500 [3:07:01<17:59, 24.54s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 457/500 [3:07:26<17:35, 24.54s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 458/500 [3:07:49<16:57, 24.22s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 459/500 [3:08:08<15:24, 22.54s/it]{'eval_loss': 2.6198716163635254, 'eval_bleu': 17.1354, 'eval_gen_len': 13.0217, 'eval_runtime': 28.9941, 'eval_samples_per_second': 9.554, 'eval_steps_per_second': 0.103, 'epoch': 16.98}
{'loss': 0.7684, 'learning_rate': 5e-05, 'epoch': 17.65}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.66s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.79s/it][A                                                   
                                             [A 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 459/500 [3:08:37<15:24, 22.54s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.79s/it][A
                                             [A 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 460/500 [3:08:59<20:44, 31.12s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 461/500 [3:09:22<18:38, 28.68s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 462/500 [3:09:51<18:16, 28.87s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 463/500 [3:10:15<16:49, 27.29s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 464/500 [3:10:40<15:52, 26.45s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 465/500 [3:11:03<14:50, 25.44s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 466/500 [3:11:28<14:19, 25.29s/it] 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 467/500 [3:11:49<13:16, 24.13s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 468/500 [3:12:18<13:36, 25.52s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 469/500 [3:12:41<12:47, 24.77s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 470/500 [3:13:05<12:16, 24.55s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 471/500 [3:13:28<11:38, 24.07s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 472/500 [3:13:53<11:21, 24.36s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 473/500 [3:14:17<10:59, 24.42s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 474/500 [3:14:43<10:45, 24.81s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 475/500 [3:15:07<10:17, 24.69s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 476/500 [3:15:28<09:25, 23.58s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 477/500 [3:15:50<08:46, 22.87s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 478/500 [3:16:15<08:40, 23.67s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 479/500 [3:16:38<08:12, 23.43s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 480/500 [3:17:00<07:38, 22.92s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 481/500 [3:17:23<07:16, 22.97s/it] 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 482/500 [3:17:43<06:39, 22.21s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 483/500 [3:18:07<06:27, 22.77s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 484/500 [3:18:28<05:55, 22.21s/it]{'eval_loss': 2.639671802520752, 'eval_bleu': 16.735, 'eval_gen_len': 13.0325, 'eval_runtime': 29.0575, 'eval_samples_per_second': 9.533, 'eval_steps_per_second': 0.103, 'epoch': 18.0}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.58s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.73s/it][A                                                   
                                             [A 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 484/500 [3:19:03<05:55, 22.21s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.73s/it][A
                                             [A 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 485/500 [3:19:15<07:23, 29.56s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 486/500 [3:19:41<06:37, 28.38s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 487/500 [3:20:03<05:43, 26.44s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 488/500 [3:20:22<04:52, 24.36s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 489/500 [3:20:44<04:21, 23.77s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 490/500 [3:21:08<03:57, 23.74s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 491/500 [3:21:32<03:33, 23.68s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 492/500 [3:21:54<03:05, 23.16s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 493/500 [3:22:16<02:41, 23.06s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 494/500 [3:22:41<02:20, 23.45s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 495/500 [3:23:08<02:03, 24.72s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 496/500 [3:23:32<01:36, 24.21s/it] 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 497/500 [3:23:54<01:10, 23.60s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 498/500 [3:24:18<00:47, 23.81s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 499/500 [3:24:43<00:24, 24.21s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [3:25:12<00:00, 25.69s/it]{'eval_loss': 2.642308473587036, 'eval_bleu': 16.405, 'eval_gen_len': 13.0036, 'eval_runtime': 29.298, 'eval_samples_per_second': 9.455, 'eval_steps_per_second': 0.102, 'epoch': 18.98}

  0%|          | 0/3 [00:00<?, ?it/s][A
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.61s/it][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.75s/it][A                                                   
                                             [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [3:25:41<00:00, 25.69s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.75s/it][A
                                             [A                                                   100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [3:25:41<00:00, 25.69s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500/500 [3:25:41<00:00, 24.68s/it]
{'eval_loss': 2.6469759941101074, 'eval_bleu': 16.4862, 'eval_gen_len': 13.0108, 'eval_runtime': 28.9975, 'eval_samples_per_second': 9.553, 'eval_steps_per_second': 0.103, 'epoch': 19.61}
{'train_runtime': 12341.8155, 'train_samples_per_second': 5.242, 'train_steps_per_second': 0.041, 'train_loss': 1.270125274658203, 'epoch': 19.61}
  0%|          | 0/3 [00:00<?, ?it/s] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:13<00:06,  6.63s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  4.76s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:15<00:00,  5.17s/it]
[LOG YJ] Trainer COMPLETE ALL JOBS HAVE DONE!
:: End ::
