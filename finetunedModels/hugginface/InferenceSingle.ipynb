{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d649fe4e-2a19-40ec-807f-a3543ce1ce97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "VAdepthENV               /home/013907062/.conda/envs/VAdepthENV\n",
      "env_onmttf               /home/013907062/.conda/envs/env_onmttf\n",
      "koen_base                /home/013907062/.conda/envs/koen_base\n",
      "newDepth                 /home/013907062/.conda/envs/newDepth\n",
      "test                     /home/013907062/.conda/envs/test\n",
      "wmt_infer             *  /home/013907062/.conda/envs/wmt_infer\n",
      "base                     /opt/ohpc/pub/apps/anaconda/3.9\n",
      "stylegan2                /opt/ohpc/pub/apps/anaconda/3.9/envs/stylegan2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b76350f9-43a5-4779-8ce5-9f46562fbcc3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, MarianMTModel, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from easydict import EasyDict\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33311401-7d7f-441e-bd4f-2520a4d1e8f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>ko</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Once upon a time, there were three beautiful b...</td>\n",
       "      <td>옛날 옛적에, 세 마리의 예쁜 나비가 있었어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I felt like I have millions butterflies in my ...</td>\n",
       "      <td>너무 긴장 한 것 같았어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The deal was completely open and above board.</td>\n",
       "      <td>거래는 완전히 공개되었고 명백했습니다.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I left the keys on the board on your porch.</td>\n",
       "      <td>현관 게시판에 열쇠를 두고 왔어요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>clean your toys in the living room. no buts!</td>\n",
       "      <td>어서 거실에 있는 장난감 정리하세요. 토 달지 말고!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en   \n",
       "0  Once upon a time, there were three beautiful b...  \\\n",
       "1  I felt like I have millions butterflies in my ...   \n",
       "2      The deal was completely open and above board.   \n",
       "3        I left the keys on the board on your porch.   \n",
       "4       clean your toys in the living room. no buts!   \n",
       "\n",
       "                              ko  \n",
       "0     옛날 옛적에, 세 마리의 예쁜 나비가 있었어요.  \n",
       "1                너무 긴장 한 것 같았어요.  \n",
       "2          거래는 완전히 공개되었고 명백했습니다.  \n",
       "3            현관 게시판에 열쇠를 두고 왔어요.  \n",
       "4  어서 거실에 있는 장난감 정리하세요. 토 달지 말고!  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_test = pd.read_csv(\"idioms__test.csv\")\n",
    "display(df_test.head())\n",
    "df_test_en_list = df_test['en'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9fd4bc1b-3b61-4d86-ad86-47d4138eb8f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read config.yaml file\n",
    "with open(\"config.yaml\") as infile:\n",
    "    SAVED_CFG = yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    CFG = EasyDict(SAVED_CFG[\"CFG\"])\n",
    "\n",
    "src_text = df_test_en_list\n",
    "result_path = \"./results/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(result_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fa7ea8b-80d0-4970-b155-491358d6119d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "translated = model.generate(\n",
    "    **tokenizer(src_text, return_tensors=\"pt\", padding=True, max_length=CFG.max_token_length,),\n",
    "    max_length=CFG.max_token_length,\n",
    "    num_beams=CFG.num_beams,\n",
    "    repetition_penalty=CFG.repetition_penalty,\n",
    "    no_repeat_ngram_size=CFG.no_repeat_ngram_size,\n",
    "    num_return_sequences=CFG.num_return_sequences,\n",
    ")\n",
    "\n",
    "#print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f2d3c64-6c09-45bf-9893-36dd4f63c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = []\n",
    "for t in translated:\n",
    "    output.append(tokenizer.decode(t, skip_special_tokens=True))\n",
    "    \n",
    "df_test['predictions'] = output\n",
    "df_test.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0a1cb8-00c5-4e84-8d0e-9b4188d7bd38",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dcd38df5-a3b9-41d1-9fff-f2acb8e6ad4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "src_text = [\"The man had egg on him today as well as yesterday.\",\n",
    "            \"I am peachy\",\n",
    "           \"He started new business one year ago. As I know it, he has made a lot of dough.\",\n",
    "           \"There's something odd about him, but I can't quite put my finger on it.\",\n",
    "           \"She didn’t know what was causing the problem, but she finally put her finger on it.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08fc73fe-99e2-4c68-bca7-985a3353ac45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['그 남자는 어제뿐만 아니라 오늘에도 그에게 의심했습니다.', '나는 기분이 좋아.', '그는 1년 전에 새로운 사업을 시작했습니다. 제가 알고 있는 바에 따르면, 그는 많은 돈을 벌었습니다.', '그에게 이상한 점이 있지만, 저는 그 점에 대해 잘 알 수 없습니다.', '그녀는 무엇이 문제를 일으키는지 몰랐지만, 마침내 그 문제에 손을 들었다.']\n"
     ]
    }
   ],
   "source": [
    "translated = model.generate(\n",
    "    **tokenizer(src_text, return_tensors=\"pt\", padding=True, max_length=CFG.max_token_length,),\n",
    "    max_length=CFG.max_token_length,\n",
    "    num_beams=CFG.num_beams,\n",
    "    repetition_penalty=CFG.repetition_penalty,\n",
    "    no_repeat_ngram_size=CFG.no_repeat_ngram_size,\n",
    "    num_return_sequences=CFG.num_return_sequences,\n",
    ")\n",
    "\n",
    "print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc22ed12-c153-4ec0-98f7-9a0670efd498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는 그것을 알아서 할 거야.', '저는 가슴이 두근두근합니다.', '콘서트가 시작되자 관중들은 열광했습니다.', '나는 시험 전에 기분이 좋아졌습니다.', '내 전화기가 해킹됐을 때 상황이 빠르게 악화되었습니다.']\n"
     ]
    }
   ],
   "source": [
    "src_text = [\"I will play it by ear.\",\n",
    "            \"I've got butterflies in my stomach.\",\n",
    "            \"The crowd went bananas when the concert began.\",\n",
    "            \"I used to get butterflies in my stomach before the tests.\",\n",
    "            \"Things quickly went south when my phone got hacked.\"]\n",
    "\n",
    "translated = model.generate(\n",
    "    **tokenizer(src_text, return_tensors=\"pt\", padding=True, max_length=CFG.max_token_length,),\n",
    "    max_length=CFG.max_token_length,\n",
    "    num_beams=CFG.num_beams,\n",
    "    repetition_penalty=CFG.repetition_penalty,\n",
    "    no_repeat_ngram_size=CFG.no_repeat_ngram_size,\n",
    "    num_return_sequences=CFG.num_return_sequences,\n",
    ")\n",
    "\n",
    "print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b137cc3-f5b7-4d39-bdfc-c0203030b409",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b76d9a5-7bb0-4831-84c5-05cb6c36e796",
   "metadata": {
    "tags": []
   },
   "source": [
    "## En Ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d92bc72-a74c-4bd9-ae7c-72fa5e8d933c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['나는 그것을 귀로 연주할 것입니다.', '저는 배에 나비가 생겼어요.', '콘서트가 시작되자 관중들은 바나나를 먹었다.', '나는 시험 전에 배에서 나비를 당하기도 했어요.', '내 핸드폰이 해킹을 당하자 상황이 빠르게 남갔다.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"config_enko.yaml\") as infile:\n",
    "    SAVED_CFG = yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    CFG = EasyDict(SAVED_CFG[\"CFG\"])\n",
    "    \n",
    "\n",
    "model_name = \"QuoQA-NLP/KE-T5-En2Ko-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "src_text = [\"I will play it by ear.\",\n",
    "            \"I've got butterflies in my stomach.\",\n",
    "            \"The crowd went bananas when the concert began.\",\n",
    "            \"I used to get butterflies in my stomach before the tests.\",\n",
    "            \"Things quickly went south when my phone got hacked.\"]\n",
    "\n",
    "translated = model.generate(\n",
    "    **tokenizer(src_text, return_tensors=\"pt\", padding=True, max_length=CFG.max_token_length,),\n",
    "    max_length=CFG.max_token_length,\n",
    "    num_beams=CFG.num_beams,\n",
    "    repetition_penalty=CFG.repetition_penalty,\n",
    "    no_repeat_ngram_size=CFG.no_repeat_ngram_size,\n",
    "    num_return_sequences=CFG.num_return_sequences,\n",
    ")\n",
    "\n",
    "print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c9268077-3d6d-41c5-9a25-94a53dda876a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['제가 유동적으로 조정할 것입니다.', '가슴이 두근두근합니다.', '콘서트가 시작되었을 때 군중은 열광했습니다.', '시험 전에 너무 떨렸어.', '제 전화기가 해킹당했을 때 상황이 빠르게 악화되었습니다.']\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./KE-T5-En2Ko-Base-finetuned-en-to-ko/checkpoint-150/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "src_text = [\"I will play it by ear.\",\n",
    "            \"I've got butterflies in my stomach.\",\n",
    "            \"The crowd went bananas when the concert began.\",\n",
    "            \"I used to get butterflies in my stomach before the tests.\",\n",
    "            \"Things quickly went south when my phone got hacked.\"]\n",
    "\n",
    "translated = model.generate(\n",
    "    **tokenizer(src_text, return_tensors=\"pt\", padding=True, max_length=CFG.max_token_length,),\n",
    "    max_length=CFG.max_token_length,\n",
    "    num_beams=CFG.num_beams,\n",
    "    repetition_penalty=CFG.repetition_penalty,\n",
    "    no_repeat_ngram_size=CFG.no_repeat_ngram_size,\n",
    "    num_return_sequences=CFG.num_return_sequences,\n",
    ")\n",
    "\n",
    "print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "51873058-718f-4bab-8edc-babd293fad60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['제가 유동적으로 조정할 것입니다.', '가슴이 두근두근합니다.', '콘서트가 시작했을 때 군중은 열광했습니다.', '시험 전에 너무 긴장해서 긴장을 많이 했어요.', '제 전화기가 해킹당했을 때 상황이 빠르게 악화되었습니다.', '우리는 가족을 만나기 위해 남쪽으로 갔어.']\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./results_enko_e10/\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "src_text = [\"I will play it by ear.\",\n",
    "            \"I've got butterflies in my stomach.\",\n",
    "            \"The crowd went bananas when the concert began.\",\n",
    "            \"I used to get butterflies in my stomach before the tests.\",\n",
    "            \"Things quickly went south when my phone got hacked.\",\n",
    "            \"we went south to meet family.\"]\n",
    "\n",
    "translated = model.generate(\n",
    "    **tokenizer(src_text, return_tensors=\"pt\", padding=True, max_length=CFG.max_token_length,),\n",
    "    max_length=CFG.max_token_length,\n",
    "    num_beams=CFG.num_beams,\n",
    "    repetition_penalty=CFG.repetition_penalty,\n",
    "    no_repeat_ngram_size=CFG.no_repeat_ngram_size,\n",
    "    num_return_sequences=CFG.num_return_sequences,\n",
    ")\n",
    "\n",
    "print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b1bed-5a94-424f-9157-1f82afb8075e",
   "metadata": {},
   "source": [
    "## Ko En"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e260b9a-2c6e-4bf7-86a4-bf6f44485607",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/013907062/.conda/envs/wmt_infer/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2436: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"It's Yuyusangjong.\", \"Don't fill it up and clean it up quickly!\", \"I can't help you because my nose is a stone.\", \"It's a dysphagia.\", \"There is a sun in the mouse hole, let's try hard.\", \"It's a case where Yeongchul is in a full stream.\", 'Food is eating porridge.']\n"
     ]
    }
   ],
   "source": [
    "# Read config.yaml file\n",
    "with open(\"config_koen.yaml\") as infile:\n",
    "    SAVED_CFG = yaml.load(infile, Loader=yaml.FullLoader)\n",
    "    CFG = EasyDict(SAVED_CFG[\"CFG\"])\n",
    "\n",
    "model_name = \"QuoQA-NLP/KE-T5-Ko2En-Base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "src_text = ['유유상종입니다.', '토 달지 말고 얼른 청소해!', '내 코가 석자라 도와 줄 수가 없네요', '진퇴양란이다.' , \n",
    "            '쥐구멍에도 볕 들 날 있다고, 우리 열심히 해 봅시다.', '영철이 완전 개천에서 용난 케이스야.', '식은 죽 먹기다.' ]\n",
    "\n",
    "translated = model.generate(\n",
    "    **tokenizer(src_text, return_tensors=\"pt\", padding=True, max_length=CFG.max_token_length,),\n",
    "    max_length=CFG.max_token_length,\n",
    "    num_beams=CFG.num_beams,\n",
    "    repetition_penalty=CFG.repetition_penalty,\n",
    "    no_repeat_ngram_size=CFG.no_repeat_ngram_size,\n",
    "    num_return_sequences=CFG.num_return_sequences,\n",
    ")\n",
    "\n",
    "print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20e843b7-7d77-4524-ba6e-91b5e6d20bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Birds of a feather flock together.', \"Don't cry over it, just clean it up!\", \"I have my own fish to fry, so I can't help you.\", \"It's between the devil and the deep blue sea.\", \"Every dog has his day, so let's try hard.\", 'Yeongchul is a case of rags to riches.', \"It's a piece of cake.\"]\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./results_Ko2En\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "src_text = ['유유상종입니다.', '토 달지 말고 얼른 청소해!', '내 코가 석자라 도와 줄 수가 없네요', '진퇴양란이다.' , \n",
    "            '쥐구멍에도 볕 들 날 있다고, 우리 열심히 해 봅시다.', '영철이 완전 개천에서 용난 케이스야.', '식은 죽 먹기다.' ]\n",
    "\n",
    "translated = model.generate(\n",
    "    **tokenizer(src_text, return_tensors=\"pt\", padding=True, max_length=CFG.max_token_length,),\n",
    "    max_length=CFG.max_token_length,\n",
    "    num_beams=CFG.num_beams,\n",
    "    repetition_penalty=CFG.repetition_penalty,\n",
    "    no_repeat_ngram_size=CFG.no_repeat_ngram_size,\n",
    "    num_return_sequences=CFG.num_return_sequences,\n",
    ")\n",
    "\n",
    "print([tokenizer.decode(t, skip_special_tokens=True) for t in translated])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621b497f-82cb-4ed2-ab11-c3969842c773",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
